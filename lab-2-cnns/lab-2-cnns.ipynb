{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Your first Convolutional Neural Network\n",
    "\n",
    "In today's lab session we will \n",
    "\n",
    "1. [Build a Shallow Convolutional Neural Network (CNN)](#Building-a-Shallow-Convolutional-Neural-Network)\n",
    "2. [Train that network on CIFAR-10](#Training-on-CIFAR-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Interactive notebooks\n",
    "\n",
    "### Running on Google Colaboratory\n",
    "\n",
    "Head over to https://colab.research.google.com and sign in with a Google account.\n",
    "\n",
    "You should see something similar to the image below:\n",
    "\n",
    "<img src=\"./media/colaboratory.png\" width=\"900\">\n",
    "\n",
    "Go to *File > New Python 3 Notebook*, it should prompt you to sign in with your google account.\n",
    "\n",
    "---\n",
    "## Running on the lab machine\n",
    "Alternatively, you can run these notebooks on the lab machines locally. First, you will need to load anaconda by entering the following into a terminal: `module load anaconda`\n",
    "\n",
    "Now, install tensorboard by entring the following into the terminal: `pip install tensorboard`\n",
    "\n",
    "With this complete, run the jupyter notebook server by entering the following into the terminal: `jupyter notebook` and navigate to `http://localhost:8888` in your browser.\n",
    "\n",
    "Go to *New > Notebook* to create a new notebook.\n",
    "\n",
    "In the first cell type the following\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.__version__\n",
    "```\n",
    "\n",
    "and click the play button to the left of the cell to run the code (Alternatively, pressing `<Ctrl>-<Enter>` will also run the code).\n",
    "\n",
    "<img src=\"./media/colaboratory-notebook.png\" width=\"700\">\n",
    "\n",
    "You should get *at least* version `1.1.0`, more likely you will get `2.4.0`.\n",
    "\n",
    "You can add new cells to the notebook by clicking the *+ Code* button in the toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Building a Shallow Convolutional Neural Network\n",
    "\n",
    "We'll be building a shallow Convolutional Neural Network (CNN) of two layers.\n",
    "\n",
    "We'll be making heavy use of pytorch's layers today, defined in the [`torch.nn`](https://pytorch.org/docs/1.2.0/nn.html) module. \n",
    "\n",
    "**Task:** Open the documentation for the fully connected layer ([`nn.Linear`](https://pytorch.org/docs/1.2.0/nn.html#linear) and 2D convolutional layer ([`nn.Conv2d`](https://pytorch.org/docs/1.2.0/nn.html#conv2d)), you'll need these later.\n",
    "\n",
    "Optimizers are defined in [`torch.optim`](https://pytorch.org/docs/1.2.0/optim.html). We'll use the [`SGD`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.SGD) optimizer like we used in the first lab.\n",
    "\n",
    "[Loss functions](https://pytorch.org/docs/1.2.0/nn.html#loss-functions) are also part of [`torch.nn`](https://pytorch.org/docs/1.2.0/nn.html#loss-functions) module. Also find the documentation for `nn.CrossEntropyLoss` (we used this last week) -- you'll need this later.\n",
    "\n",
    "We will implement the following architecture, as described in your practical intro slides.\n",
    "\n",
    "<img alt=\"CNN Architecture diagram\" src=\"./media/cnn-ex-8.png\" style=\"max-height: 300px;\">\n",
    "\n",
    "This diagram is drawn in the style put forward in the [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) where inputs/outputs are visualised as 3D volumes and layers are implicit between the inputs/outputs with only the receptive fields drawn (highlighted in orange, pink, and blue in the figure above).\n",
    "\n",
    "The output shapes of each layer are listed in the table below\n",
    "\n",
    "| Layer  | Output shape ($C \\times H \\times W$) |\n",
    "|--------|------------------------------|\n",
    "| Input  | $3 \\times 32 \\times 32$      |\n",
    "| Conv1  | $32 \\times 32 \\times 32$     |\n",
    "| Pool1  | $32 \\times 16 \\times 16$     |\n",
    "| Conv2  | $64\\times 16 \\times 16$     |\n",
    "| Pool2  | $64 \\times 8 \\times 8$       |\n",
    "| FC1    | $1024$                       |\n",
    "| FC2    | $10$                         |\n",
    "\n",
    "Our network is designed to operate on images from CIFAR-10, a dataset containing 60,000 RGB images, each 32 $\\times$ 32 in resolution, split into 50,000 images for training and 10,000 images for testing. \n",
    "\n",
    "There are 10 classes with 6,000 examples per class. Some examples of each class can be seen in the diagram below\n",
    "\n",
    "<img alt=\"CIFAR-10 examples\" src=\"./media/cifar10.png\" style=\"max-height: 500px;\" />\n",
    "\n",
    "\n",
    "We've provided you with a boilerplate script `train_cifar.py` to get you started. Download the code to your laptop by cloning the git repository  to your laptop:\n",
    "\n",
    "```console\n",
    "$ git clone https://github.com/COMSM0045-Applied-Deep-Learning/labsheets.git\n",
    "```\n",
    "\n",
    "If you don't have git, then download a zip copy via the green button in the top right of https://github.com/COMSM0045-Applied-Deep-Learning/labsheets.\n",
    "\n",
    "\n",
    "The code provided is in `lab-2-cnns/lab2-code/`. There are two files:\n",
    "- `train_cifar.py`: This contains the code that you will need to edit in this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll draw the architecture of the CNN bit by bit, accompanying it by code to show you how to implement the first few layers. We'll leave you to implement the rest. \n",
    "\n",
    "First up is the input to the network, this is a single input image drawn as a 3D volume:\n",
    "\n",
    "<img alt=\"Input shape\" src=\"./media/cnn-ex-1.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "In PyTorch, our network definition is split into two parts: The first part allocates the memory for the parameterized layers and takes place in the constructor, the second part defines the forward pass defining how the input data flows through the layers.\n",
    "\n",
    "In this snippet of code from `train_cifar.py`, we define the bare skeleton of the CNN object. It has a few constructor arguments that define the shape of the input which we store in an `ImageShape` data structure.\n",
    "\n",
    "```python\n",
    "class ImageShape(NamedTuple):\n",
    "    height: int\n",
    "    width: int\n",
    "    channels: int\n",
    "\n",
    "        \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 height: int = 32,\n",
    "                 width: int = 32,\n",
    "                 channels: int = 3,\n",
    "                 class_count: int = 10):\n",
    "        self.input_shape = ImageShape(height, width, channels)\n",
    "        self.class_count = class_count\n",
    "            ...\n",
    "```\n",
    "\n",
    "Our first layer sits in between the input tensor and the output tensor. One of the convolutional filter's receptive field is visualised by the orange cube in the input. Once the filter has been convolved with the input at one position, it produces a single value in the output tensor, indicated by the tip of the orange pyramid. The depth (horizontal) of the output tensor indicates the number of convolutional filters of the layer, in this case that is 32, i.e. there are 32 different orange cubes (different filter weights) convolved with the input.\n",
    "\n",
    "<img alt=\"First conv layer\" src=\"./media/cnn-ex-2.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "We define the convolutional layer as an attribute in the constructor, and pass the `images` through it during the `forward` pass.\n",
    "\n",
    "Here is a webpage that visualises how the convolution layers, filters, and activation functions: [https://poloclub.github.io/cnn-explainer/#:~:text=Understanding%20Hyperparameters](https://poloclub.github.io/cnn-explainer/#:~:text=Understanding%20Hyperparameters)\n",
    "\n",
    "```python\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 height: int = 32,\n",
    "                 width: int = 32,\n",
    "                 channels: int = 3,\n",
    "                 class_count: int = 10):\n",
    "        ...\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=self.input_shape.channels,\n",
    "            out_channels=32,\n",
    "            kernel_size=(5, 5),\n",
    "            padding=(2, 2),\n",
    "        )\n",
    "        self.initialise_layer(self.conv1)\n",
    "        ...\n",
    "        \n",
    "    def forward(images: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "        x = F.relu(self.conv1(images))\n",
    "        ...\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "```\n",
    "\n",
    "Note that we apply the ReLU function in the forward pass, this is defined in the `torch.nn.functional` module (traditionally imported with the alias `F`) .\n",
    "\n",
    "We have defined a method `initialise_layer` that initialises the layer's parameters. The `bias` weight is initialised with zeros, and for the `weight` attribute we use [`kaiming_normal_`](https://pytorch.org/docs/1.2.0/nn.init.html#torch.nn.init.kaiming_uniform_) to initialise the weight matrix with values from $\\mathcal{N}(0, \\sigma^2)$ where $\\sigma$ is dependent upon the number of inputs to the layer.\n",
    "\n",
    "Next we halve the spatial dimensions of the output of the convolutional layer by 'pooling' its output by placing a $2 \\times 2$ grid at each position in the input and taking the max value within this grid. The purple grid in the figure slides across the full tensor. Note that the channel dimension stays the same--that's because pooling is applied for *each* channel dimension.\n",
    "\n",
    "<img alt=\"First pooling layer\" src=\"./media/cnn-ex-3.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "```python\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 height: int = 32,\n",
    "                 width: int = 32,\n",
    "                 channels: int = 3,\n",
    "                 class_count: int = 10):\n",
    "        ...\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        ...\n",
    "\n",
    "    def forward(images: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "        x = F.relu(self.conv1(images))\n",
    "        x = self.pool1(x)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colaboratory, you need to copy-paste the content of the file in your colaboratory notebook. \n",
    "For lab machines, you already have a GPU so you can test your code as you implement the subsequent layers.\n",
    "\n",
    "Use the following code to test the first two layers of the network:\n",
    "\n",
    "```bash\n",
    "$ cd lab2-code\n",
    "$ python train_cifar.py\n",
    "```\n",
    "\n",
    "**BEWARE**: the code won't yet run without crashing as you need to implement the rest of the network and training process; what we've provided is just skeleton code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your turn now to implement the remaining CNN layers. We're going to help you approach this in a step-by-step manner so you can verify your progress as you add each layer. To accomplish this, we'll print out the output shape of the network as we add each layer. This is useful because it shows you the output shape of the last layer you implemented, and consequently the input shape of the next one you need to implement.\n",
    "\n",
    "**Task 1:** Within the training loop in the `Trainer.train` method, compute the model's forward pass using `self.model` on the `batch`. `batch` contains a $N \\times 3 \\times 32 \\times 32$ batch of CIFAR-10 images. Assign the result of the forward pass to a local variable named `output`. Finally, print the output shape, and quit the program.\n",
    "\n",
    "```python\n",
    "## TASK 1: Compute the forward pass of the model, print the output shape\n",
    "##         and quit the program\n",
    "output = self.model.forward(batch)\n",
    "print(output.shape)\n",
    "import sys; sys.exit(1)\n",
    "```            \n",
    "\n",
    "Run your code with the command `python train_cifar.py`. You're program should print the output shape of the first conv layer:\n",
    "\n",
    "```python\n",
    "torch.Size([128, 32, 16, 16])\n",
    "```\n",
    "\n",
    "The data layout is in `NCHW` format, where \n",
    "\n",
    "- `N` is the batch size\n",
    "- `C` is the channel depth\n",
    "- `H` is the height\n",
    "- `W` is the width\n",
    "\n",
    "The batch size is 128 in this case, the spatial dimensions are $16 \\times 16$, and the channel depth is 32.\n",
    "\n",
    "**Task 2:** Now add the second convolutional layer, it has a $5 \\times 5$ kernel, \n",
    "Don't forget to pass the output through `relu` in the `forward` function. Run the code and verify that the output shape against the diagram.\n",
    "\n",
    "<img alt=\"Second conv layer\" src=\"./media/cnn-ex-4.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "You'll need to add code below the following comments in your script\n",
    "```python\n",
    "## TASK 2-1: Define the second convolutional layer and initialise its parameters.\n",
    "## Hint: copy the code for conv1, changing the name as well as the arguments for in_channels and out_channels. Also remember to initialise this layer!\n",
    "``` \n",
    "and\n",
    "```python\n",
    "## TASK 2-2: Pass x through the second convolutional layer\n",
    "## Hint: Don't forget to pass it through a relu after the convolution. \n",
    "```\n",
    "\n",
    "**Task 3:** Next add the second pooling layer. Again you need to define this layer at `## Task 3-1` before calling this layer at `## Task 3-2`.  Run the code and verify the output shape.\n",
    "\n",
    "<img alt=\"Second pooling layer\" src=\"./media/cnn-ex-5.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "**Task 4:** Flatten the tensor produced by the second pooling layer from $8 \\times 8 \\times 64$ to $4096$, use [`torch.flatten`](https://pytorch.org/docs/1.2.0/torch.html) for this; take special note of the `start_dim` kwarg. You need to set this to 1 (default is 0) to avoid flattening the whole batch. All the functions in Pytorch expect a tensor of shape $N \\times \\ldots$ where $N$ is the batch size. By setting `start_dim` to 1, we flatten each image, not the whole batch. Run the code and check your output is a 2D tensor, first dimension is the batch size, and the second should be 4096.\n",
    "\n",
    "<img alt=\"Flattened convolutional features\" src=\"./media/cnn-ex-6.png\" style=\"max-height: 300px;\">\n",
    "\n",
    "**Task 5:** Now take the flattened features and pass them through a fully connected layer (a.k.a a [`Linear`](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Linear) layer in PyTorch) mapping the $4096$ feature to $1024$. Copy the code\n",
    "\n",
    "```python\n",
    "## TASK 5-1: Define the first FC layer and initialise its parameters\n",
    "self.fc1 = nn.Linear(4096, 1024)\n",
    "self.initialise_layer(self.fc1)\n",
    "```\n",
    "Now add code to use this layer in the forward pass under (don't forget the ReLU activation function after the fully connected layer)\n",
    "```python\n",
    "## TASK 5-2: Pass x through the first fully connected layer\n",
    "```\n",
    "Run the code and check the output shape is `(128, 1024)`.\n",
    "\n",
    "<img alt=\"First FC layer\" src=\"./media/cnn-ex-7.png\" style=\"max-height: 300px;\">\n",
    "\n",
    "**Task 6:** Add the final fully connected layer that maps from the $1024$ feature to the number of classes. This layer produces our [*logits*](https://developers.google.com/machine-learning/glossary/#logits), the unbounded class scores (**do NOT** use ReLU after this layer). Run the code and check the output shape is `(128, 10)`.\n",
    "\n",
    "<img alt=\"Final FC layer\" src=\"./media/cnn-ex-8.png\" style=\"max-height: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training on CIFAR-10\n",
    "\n",
    "Now that you've defined your network we can go ahead and train it. To do so we need a loss function and an optimizer. For our loss function we'll use softmax cross entropy, the standard loss function for single-label classification tasks. For our optimizer we'll use Stochastic Gradient Descent (SGD). \n",
    "\n",
    "**Task 7:** Rename the `output` variable in he training loop of the `Trainer.train` method where you store you model output to `logits` as the model now produces a logit vector of class scores; the subsequent code in `Trainer.train` depends on this variable. \n",
    "\n",
    "Also remove the code you previously wrote to print the output and exit\n",
    "```python\n",
    "print(output.shape)\n",
    "import sys; sys.exit(1)\n",
    "```\n",
    "\n",
    "**Task 8:** In the `main(args)` function, replace\n",
    "```python\n",
    "## TASK 8: Redefine the criterion to be softmax cross entropy\n",
    "criterion = lambda logits, labels: torch.tensor(0)\n",
    "```\n",
    "with\n",
    "```python\n",
    "## TASK 8: Redefine the criterion to be softmax cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "This defines the softmax cross entropy loss [`nn.CrossEntropyLoss`](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.CrossEntropyLoss). You will need to remove the dummy loss code `lambda logits, labels: torch.tensor(0)`, which was only there so you could run your code in previous steps to check your progress so far.\n",
    "\n",
    "**Task 9:** Back in the training loop, replace the dummy loss function\n",
    "```python\n",
    "## TASK 9: Compute the loss using self.criterion and\n",
    "##         store it in a variable called `loss`\n",
    "loss = torch.tensor(0)\n",
    "```\n",
    "with your loss computed using `self.criterion`. This takes two arguments: the `logits` and the local variable `labels` (a 1D vector of length $N$ containing the labels corresponding to each example in the batch).\n",
    "\n",
    "Run the code and check the loss printed out matches chance performance. Accuracy chance is 10% for randomly selecting one out of 10 classes. You don't need to run the code until completion, you can kill it after you've checked performance for a few batches by pressing `<Ctrl-C>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have **not** yet trained our model. This was simply the forward pass.\n",
    " \n",
    "**Task 10:** Compute the backward pass (this populates the gradient buffers of your network parameters) by calling `backward` on your `loss` variable (i.e. `loss.backward()`). \n",
    "\n",
    "**Task 11:** Initialise the variable named `optimizer` in the `main` function to a [`torch.optim.SGD`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.SGD) object using the [parameters of your model](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Module.parameters) and the learning rate stored in `args.learning_rate`.\n",
    "\n",
    "**Task 12:** Update your network's parameters by calling [`self.optimizer.step()`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.Optimizer.step) and zero-out your gradient buffers using [`self.optimizer.zero_grad()`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.Optimizer.zero_grad) in `Trainer.train`. Run your code and check that your model's accuracy during training and testing increases and the loss decreases.\n",
    "\n",
    "As you watch the model trained, you should see the accuracy increasing from chance (10%) to around **65%** by the end of epoch 20. Note the difference between your `batch accuracy` and your overall `accuracy` on the full test set.\n",
    "\n",
    "```\n",
    "epoch: [19], step: [390/391], batch loss: 0.76462, batch accuracy: 72.50, data load time: 0.00019, step time: 0.00363\n",
    "validation loss: 0.95978, accuracy: 66.66\n",
    "```\n",
    "\n",
    "For example, at the end of my training, the batch loss for the last batch was 0.76 and the batch accuracy 72.5%.\n",
    "However when testing the model on the full test set, the accuracy was 66.6% with the loss 0.96.\n",
    "\n",
    "Q. Why is the batch accuracy different from the overall accuracy?\n",
    "\n",
    "A. The batch accuracy is computed on a small subset of the data (128 images) whereas the overall accuracy is computed on the full test set (10,000 images).\n",
    "\n",
    "Q. If you re-run the training do you get the same training accuracy? If not, why? What about the test accuracy?\n",
    "\n",
    "A. Because in each iteration, the network predicts the classes for a different batch of examples (a set of 128 randomly selected images). There is shuffling in the data loader, so the network sees the data in a different order each time. The test accuracy should be more stable as it is computed on the full test set, however, it may still vary slightly due to the random initialisation of the network weights and the randomness of SGD.\n",
    "\n",
    "Q. Let's say you want to use the model deterministically in some real-world application. How do you think that is possible?\n",
    "\n",
    "A. We could _save_ the model parameters and then load them later to use the _same_ parameters without running training again. This is called _checkpointing_.\n",
    "\n",
    "---\n",
    "\n",
    "In addition to the metrics being printed to the console, the code we've provided also logs values to a tensorboard log directory. Similarly to lab-1, we're going to launch a tensorboard server and visualise the training and validation curves.\n",
    "\n",
    "Each time you run your code, a new subdirectory containing that run's logs will be saved within the `logs` directory. You will see a bunch of subdirectorys already within `logs` when you were testing your code earlier as you built your network.\n",
    "Open a new terminal and navigate to the directory containing the `logs` directory.\n",
    "\n",
    "Now, run the following commands \n",
    "```bash\n",
    "module load anaconda\n",
    "python3 -m tensorboard.main --logdir=/path/to/logs\n",
    "```\n",
    "on this terminal.\n",
    "\n",
    "\n",
    "**Note** If you get an issue with `tensorboard: command not found`, \n",
    "```console\n",
    "pip install tensorboard\n",
    "```\n",
    "to load it.\n",
    "\n",
    "Open up http://localhost:6006. Have a look at the loss and accuracy curves plotted for both training and test.\n",
    "\n",
    "The x-axis is the number of steps we've trained for.\n",
    "\n",
    "By default tensorboard smoothes your data by computing a running average. You can adjust this smoothing using the slider in the left side bar. We'd recommend turning this off to begin with as the smoothing can be deceptive and hide issues with training.\n",
    "\n",
    "Make sure your tensorboard settings match those in the screenshot below:\n",
    "\n",
    "![Tensorboard settings](./media/tensorboard-settings.png)\n",
    "\n",
    "Your network should produce a similar result to the graphs shown below:\n",
    "\n",
    "If you see multiple curves, it's because you've trained the model multiple times already. Each run will produce a new log folder within `logs`, clear all but the most recent one and rerun tensorboard.\n",
    "\n",
    "![Expected accuracy](./media/expected-accuracy.png) ![Expected loss](./media/expected-loss.png)\n",
    "\n",
    "**Task 13:** Run `python train_cifar.py --help` and investigate the hyperparameters that you can tweak. Pick one hyperparameter to change, run the code again and look at how the loss and accuracy curves differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
