{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: BC4 and your first neural network\n",
    "\n",
    "In today's lab session we will \n",
    "\n",
    "1. [Build a Shallow Convolutional Neural Network (CNN)](#Building-a-Shallow-Convolutional-Neural-Network)\n",
    "2. [Train that network on CIFAR-10](#Training-on-CIFAR-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Building a Shallow Convolutional Neural Network\n",
    "\n",
    "We'll be building a shallow Convolutional Neural Network (CNN) of two layers.\n",
    "\n",
    "We'll be making heavy use of pytorch's layers today, defined in the [`torch.nn`](https://pytorch.org/docs/1.2.0/nn.html) module. \n",
    "\n",
    "**Task:** Open the documentation for the fully connected layer ([`nn.Linear`](https://pytorch.org/docs/1.2.0/nn.html#linear) and 2D convolutional layer ([`nn.Conv2d`](https://pytorch.org/docs/1.2.0/nn.html#conv2d)), you'll need these later.\n",
    "\n",
    "Optimizers are defined in [`torch.optim`](https://pytorch.org/docs/1.2.0/optim.html). We'll use the [`SGD`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.SGD) optimizer like we used in the first lab.\n",
    "\n",
    "[Loss functions](https://pytorch.org/docs/1.2.0/nn.html#loss-functions) are also part of [`torch.nn`](https://pytorch.org/docs/1.2.0/nn.html#loss-functions) module. Also find the documentation for `nn.CrossEntropyLoss` (we used this last week) -- you'll need this later.\n",
    "\n",
    "We will implement the following architecture, as described in your practical intro slides.\n",
    "\n",
    "<img alt=\"CNN Architecture diagram\" src=\"./media/cnn-ex-8.png\" style=\"max-height: 300px;\">\n",
    "\n",
    "This diagram is drawn in the style put forward in the [AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) where inputs/outputs are visualised as 3D volumes and layers are implicit between the inputs/outputs with only the receptive fields drawn (highlighted in orange, pink, and blue in the figure above).\n",
    "\n",
    "The output shapes of each layer are listed in the table below\n",
    "\n",
    "| Layer  | Output shape ($C \\times H \\times W$) |\n",
    "|--------|------------------------------|\n",
    "| Input  | $3 \\times 32 \\times 32$      |\n",
    "| Conv1  | $32 \\times 32 \\times 32$     |\n",
    "| Pool1  | $32 \\times 16 \\times 16$     |\n",
    "| Conv2  | $64\\times 16 \\times 16$     |\n",
    "| Pool2  | $64 \\times 8 \\times 8$       |\n",
    "| FC1    | $1024$                       |\n",
    "| FC2    | $10$                         |\n",
    "\n",
    "Our network is designed to operate on images from CIFAR-10, a dataset containing 60,000 RGB images, each 32 $\\times$ 32 in resolution, split into 50,000 images for training and 10,000 images for testing. \n",
    "\n",
    "There are 10 classes with 6,000 examples per class. Some examples of each class can be seen in the diagram below\n",
    "\n",
    "<img alt=\"CIFAR-10 examples\" src=\"./media/cifar10.png\" style=\"max-height: 500px;\" />\n",
    "\n",
    "\n",
    "We've provided you with a boilerplate script `train_cifar.py` to get you started. Download the code to your laptop by cloning the git repository  to your laptop:\n",
    "\n",
    "```console\n",
    "$ git clone https://github.com/COMSM0045-Applied-Deep-Learning/labsheets.git\n",
    "```\n",
    "\n",
    "If you don't have git, then download a zip copy via the green button in the top right of https://github.com/COMSM0045-Applied-Deep-Learning/labsheets.\n",
    "\n",
    "\n",
    "The code provided is in `lab-2-cnns/lab2-code/`. There are two files:\n",
    "- `train_cifar.py`: This contains the code that you will need to edit in this lab\n",
    "- `train_cifar.sh`: This is a slurm job script which you will run on BC4 to train and evaluate your network when you aren't using interactive sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll draw the architecture of the CNN bit by bit, accompanying it by code to show you how to implement the first few layers. We'll leave you to implement the rest. \n",
    "\n",
    "First up is the input to the network, this is a single input image drawn as a 3D volume:\n",
    "\n",
    "<img alt=\"Input shape\" src=\"./media/cnn-ex-1.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "In PyTorch, our network definition is split into two parts: The first part allocates the memory for the parameterized layers and takes place in the constructor, the second part defines the forward pass defining how the input data flows through the layers.\n",
    "\n",
    "In this snippet of code from `train_cifar.py`, we define the bare skeleton of the CNN object. It has a few constructor arguments that define the shape of the input which we store in an `ImageShape` data structure.\n",
    "\n",
    "```python\n",
    "class ImageShape(NamedTuple):\n",
    "    height: int\n",
    "    width: int\n",
    "    channels: int\n",
    "\n",
    "        \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 height: int = 32,\n",
    "                 width: int = 32,\n",
    "                 channels: int = 3,\n",
    "                 class_count: int = 10):\n",
    "        self.input_shape = ImageShape(height, width, channels)\n",
    "        self.class_count = class_count\n",
    "            ...\n",
    "```\n",
    "\n",
    "Our first layer sits in between the input tensor and the output tensor. One of the convolutional filter's receptive field is visualised by the orange cube in the input. Once the filter has been convolved with the input at one position, it produces a single value in the output tensor, indicated by the tip of the orange pyramid. The depth (horizontal) of the output tensor indicates the number of convolutional filters of the layer, in this case that is 32, i.e. there are 32 different orange cubes (different filter weights) convolved with the input.\n",
    "\n",
    "<img alt=\"First conv layer\" src=\"./media/cnn-ex-2.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "We define the convolutional layer as an attribute in the constructor, and pass the `images` through it during the `forward` pass.\n",
    "\n",
    "```python\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 height: int = 32,\n",
    "                 width: int = 32,\n",
    "                 channels: int = 3,\n",
    "                 class_count: int = 10):\n",
    "        ...\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=self.input_shape.channels,\n",
    "            out_channels=32,\n",
    "            kernel_size=(5, 5),\n",
    "            padding=(3, 3),\n",
    "        )\n",
    "        self.initialise_layer(self.conv1)\n",
    "        ...\n",
    "        \n",
    "    def forward(images: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "        x = F.relu(self.conv1(images))\n",
    "        ...\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def initialise_layer(layer):\n",
    "        if hasattr(layer, \"bias\"):\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        if hasattr(layer, \"weight\"):\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "```\n",
    "\n",
    "Note that we apply the ReLU function in the forward pass, this is defined in the `torch.nn.functional` module (traditionally imported with the alias `F`) .\n",
    "\n",
    "We have defined a method `initialise_layer` that initialises the layer's parameters. The `bias` weight is initialised with zeros, and for the `weight` attribute we use [`kaiming_normal_`](https://pytorch.org/docs/1.2.0/nn.init.html#torch.nn.init.kaiming_uniform_) to initialise the weight matrix with values from $\\mathcal{N}(0, \\sigma^2)$ where $\\sigma$ is dependent upon the number of inputs to the layer.\n",
    "\n",
    "Next we halve the spatial dimensions of the output of the convolutional layer by 'pooling' its output by placing a $2 \\times 2$ grid at each position in the input and taking the max value within this grid. The purple grid in the figure slides across the full tensor. Note that the channel dimension stays the same--that's because pooling is applied for *each* channel dimension.\n",
    "\n",
    "<img alt=\"First pooling layer\" src=\"./media/cnn-ex-3.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "```python\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 height: int = 32,\n",
    "                 width: int = 32,\n",
    "                 channels: int = 3,\n",
    "                 class_count: int = 10):\n",
    "        ...\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        ...\n",
    "\n",
    "    def forward(images: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "        x = F.relu(self.conv1(images))\n",
    "        x = self.pool1(x)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the code over to BC4 and get an interactive session on a GPU node so you can test your code as you implement the subsequent layers.\n",
    "\n",
    "**BEWARE**: the code won't yet run without crashing as you need to implement the rest of the network and training process; what we've provided is just skeleton code!\n",
    "\n",
    "\n",
    "Copy `train_cifar.py` file to BC4 **every time you update it on your machine**.\n",
    "\n",
    "**Linux/MacOS/WSL users:**\n",
    "\n",
    "Copy the files over to BC4\n",
    "```console\n",
    "$ cd path/to/files\n",
    "$ scp -r lab2-code bc4-external:/path/to/destination/\n",
    "```\n",
    "(If you didn't set up a `bc4-external` alias in `~/.ssh/config` last week we suggest you do this now. If not you can first copy the file to snowy with: `scp -r lab2-code <username>@snowy.cs.bris.ac.uk:~/` and then copying the file to blue crystal by first sshing into snowy `ssh <username>@snowy.cs.bris.ac.uk` and then copying the file from snowy to bluecrystal:  `scp -r lab2-code <username>@bc4login.acrc.bris.ac.uk:/path/to/destination/`.)\n",
    "\n",
    "In a **different** console, login to BC4 and get an interactive session on a GPU node\n",
    "\n",
    "```console\n",
    "$ ssh <username>@bc4login.acrc.bris.ac.uk  # connect to a BC4 login node\n",
    "[bc4] $ cd ~/lab2-code\n",
    "[bc4] $ module load languages/anaconda3/2021-3.8.8-cuda-11.1-pytorch\n",
    "[bc4] $ srun --job-name lab1 --partition teach_gpu --nodes 1 --gres gpu:1 --time 0:20:00 --mem 4GB python train_cifar.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your turn now to implement the remaining CNN layers. We're going to help you approach this in a step-by-step manner so you can verify your progress as you add each layer. To accomplish this, we'll print out the output shape of the network as we add each layer. This is useful because it shows you the output shape of the last layer you implemented, and consequently the input shape of the next one you need to implement.\n",
    "\n",
    "**Task 1:** Within the training loop in the `Trainer.train` method, compute the model's forward pass using `self.model` on the `batch`. `batch` contains a $N \\times 3 \\times 32 \\times 32$ batch of CIFAR-10 images. Assign the result of the forward pass to a local variable named `output`. Finally, print the output shape, and quit the program.\n",
    "\n",
    "```python\n",
    "## TASK 1: Compute the forward pass of the model, print the output shape\n",
    "##         and quit the program\n",
    "output = self.model.forward(batch)\n",
    "print(output.shape)\n",
    "import sys; sys.exit(1)\n",
    "```            \n",
    "\n",
    "Run your code with the command `python train_cifar.py`. You're program should print the output shape of the first conv layer:\n",
    "\n",
    "```python\n",
    "torch.Size([128, 32, 16, 16])\n",
    "```\n",
    "\n",
    "The data layout is in `NCHW` format, where \n",
    "\n",
    "- `N` is the batch size\n",
    "- `C` is the channel depth\n",
    "- `H` is the height\n",
    "- `W` is the width\n",
    "\n",
    "The batch size is 128 in this case, the spatial dimensions are $16 \\times 16$, and the channel depth is 32.\n",
    "\n",
    "**Task 2:** Now add the second convolutional layer, it has a $5 \\times 5$ kernel, \n",
    "Don't forget to pass the output through `relu` in the `forward` function. Run the code and verify that the output shape against the diagram.\n",
    "\n",
    "<img alt=\"Second conv layer\" src=\"./media/cnn-ex-4.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "You'll need to add code below the following comments in your script\n",
    "```python\n",
    "## TASK 2-1: Define the second convolutional layer and initialise its parameters.\n",
    "## Hint: copy the code for conv1, changing the name as well as the arguments for in_channels and out_channels. Also remember to initialise this layer!\n",
    "``` \n",
    "and\n",
    "```python\n",
    "## TASK 2-2: Pass x through the second convolutional layer\n",
    "## Hint: Don't forget to pass it through a relu after the convolution. \n",
    "```\n",
    "\n",
    "**Task 3:** Next add the second pooling layer. Again you need to define this layer at `## Task 3-1` before calling this layer at `## Task 3-2`.  Run the code and verify the output shape.\n",
    "\n",
    "<img alt=\"Second pooling layer\" src=\"./media/cnn-ex-5.png\" style=\"max-height: 200px;\">\n",
    "\n",
    "**Task 4:** Flatten the tensor produced by the second pooling layer from $8 \\times 8 \\times 64$ to $4096$, use [`torch.flatten`](https://pytorch.org/docs/1.2.0/torch.html) for this; take special note of the `start_dim` kwarg. You need to set this to 1 (default is 0) to avoid flattening the whole batch. All the functions in Pytorch expect a tensor of shape $N \\times \\ldots$ where $N$ is the batch size. By setting `start_dim` to 1, we flatten each image, not the whole batch. Run the code and check your output is a 2D tensor, first dimension is the batch size, and the second should be 4096.\n",
    "\n",
    "<img alt=\"Flattened convolutional features\" src=\"./media/cnn-ex-6.png\" style=\"max-height: 300px;\">\n",
    "\n",
    "**Task 5:** Now take the flattened features and pass them through a fully connected layer (a.k.a a [`Linear`](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Linear) layer in PyTorch) mapping the $4096$ feature to $1024$. Copy the code\n",
    "\n",
    "```python\n",
    "## TASK 5-1: Define the first FC layer and initialise its parameters\n",
    "self.fc1 = nn.Linear(4096, 1024)\n",
    "self.initialise_layer(self.fc1)\n",
    "```\n",
    "Now add code to use this layer in the forward pass under (don't forget the ReLU activation function after the fully connected layer)\n",
    "```python\n",
    "## TASK 5-2: Pass x through the first fully connected layer\n",
    "```\n",
    "Run the code and check the output shape is `(128, 1024)`.\n",
    "\n",
    "<img alt=\"First FC layer\" src=\"./media/cnn-ex-7.png\" style=\"max-height: 300px;\">\n",
    "\n",
    "**Task 6:** Add the final fully connected layer that maps from the $1024$ feature to the number of classes. This layer produces our [*logits*](https://developers.google.com/machine-learning/glossary/#logits), the unbounded class scores (**do NOT** use ReLU after this layer). Run the code and check the output shape is `(128, 10)`.\n",
    "\n",
    "<img alt=\"Final FC layer\" src=\"./media/cnn-ex-8.png\" style=\"max-height: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training on CIFAR-10\n",
    "\n",
    "Now that you've defined your network we can go ahead and train it. To do so we need a loss function and an optimizer. For our loss function we'll use softmax cross entropy, the standard loss function for single-label classification tasks. For our optimizer we'll use Stochastic Gradient Descent (SGD). \n",
    "\n",
    "**Task 7:** Rename the `output` variable in he training loop of the `Trainer.train` method where you store you model output to `logits` as the model now produces a logit vector of class scores; the subsequent code in `Trainer.train` depends on this variable. \n",
    "\n",
    "Also remove the code you previously wrote to print the output and exit\n",
    "```python\n",
    "print(output.shape)\n",
    "import sys; sys.exit(1)\n",
    "```\n",
    "\n",
    "**Task 8:** In the `main(args)` function, replace\n",
    "```python\n",
    "## TASK 8: Redefine the criterion to be softmax cross entropy\n",
    "criterion = lambda logits, labels: torch.tensor(0)\n",
    "```\n",
    "with\n",
    "```python\n",
    "## TASK 8: Redefine the criterion to be softmax cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "This defines the softmax cross entropy loss [`nn.CrossEntropyLoss`](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.CrossEntropyLoss). You will need to remove the dummy loss code `lambda logits, labels: torch.tensor(0)`, which was only there so you could run your code in previous steps to check your progress so far.\n",
    "\n",
    "**Task 9:** Back in the training loop, replace the dummy loss function\n",
    "```python\n",
    "## TASK 9: Compute the loss using self.criterion and\n",
    "##         store it in a variable called `loss`\n",
    "loss = torch.tensor(0)\n",
    "```\n",
    "with your loss computed using `self.criterion`. This takes two arguments: the `logits` and the local variable `labels` (a 1D vector of length $N$ containing the labels corresponding to each example in the batch).\n",
    "\n",
    "Run the code and check the loss printed out matches chance performance. Accuracy chance is 10% for randomly selecting one out of 10 classes. You don't need to run the code until completion, you can kill it after you've checked performance for a few batches by pressing `<Ctrl-C>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Why does the accuracy change over different iterations? \n",
    "\n",
    "A. Because in each iteration, the network predicts the classes for a different batch of examples (a set of 128 randomly selected images).\n",
    "\n",
    "We have **not** yet trained our model. This was simply the forward pass.\n",
    " \n",
    "**Task 10:** Compute the backward pass (this populates the gradient buffers of your network parameters) by calling `backward` on your `loss` variable (i.e. `loss.backward()`). \n",
    "\n",
    "**Task 11:** Initialise the variable named `optimizer` in the `main` function to a [`torch.optim.SGD`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.SGD) object using the [parameters of your model](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Module.parameters) and the learning rate stored in `args.learning_rate`.\n",
    "\n",
    "**Task 12:** Update your network's parameters by calling [`self.optimizer.step()`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.Optimizer.step) and zero-out your gradient buffers using [`self.optimizer.zero_grad()`](https://pytorch.org/docs/1.2.0/optim.html#torch.optim.Optimizer.zero_grad) in `Trainer.train`. Run your code and check that your model's accuracy during training and testing increases and the loss decreases.\n",
    "\n",
    "As you watch the model trained, you should see the accuracy increasing from chance (10%) to around **65%** by the end of epoch 20. Note the difference between your `batch accuracy` and your overall `accuracy` on the full test set.\n",
    "\n",
    "```\n",
    "epoch: [19], step: [390/391], batch loss: 0.76462, batch accuracy: 72.50, data load time: 0.00019, step time: 0.00363\n",
    "validation loss: 0.95978, accuracy: 66.66\n",
    "```\n",
    "\n",
    "For example, at the end of my training, the batch loss for the last batch was 0.76 and the batch accuracy 72.5%.\n",
    "However when testing the model on the full test set, the accuracy was 66.6% with the loss 0.96.\n",
    "\n",
    "---\n",
    "\n",
    "In addition to the metrics being printed to the console, the code we've provided also logs values to \n",
    "a tensorboard log directory. We're going to launch a tensorboard server on the GPU node and port forward this to your laptop so you can access it locally.\n",
    "\n",
    "Each time you run your code, a new subdirectory containing that run's logs will be saved within the `logs` directory. You will see a bunch of subdirectorys already within `logs` when you were testing your code earlier as you built your network.\n",
    "\n",
    "\n",
    "**Linux/mac/WSL** users:\n",
    "```console\n",
    "[bc4] $ PORT=$((($UID-6025) % 65274))  # compute a unique port and save to the PORT env var\n",
    "[bc4] $ echo $PORT  # echo the PORT env var to the console so you can see what it is\n",
    "<PORT>\n",
    "[bc4] $ hostname -s  # get hostname.\n",
    "<HOSTNAME>\n",
    "[bc4] $ cd ~/lab2-code  # change the working directory to where the tensorboard logs are stored\n",
    "[bc4] $ module load \"languages/anaconda3/2021-3.8.8-cuda-11.1-pytorch\"\n",
    "[bc4] $ tensorboard --logdir logs --port \"$PORT\" --bind_all # run the tensorboard server on the port stored in the PORT env var\n",
    "TensorBoard 1.14.0 at http://<HOSTNAME>.bc4.acrc.priv:<PORT>/ (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "Open another terminal on your laptop and run an SSH process to forward the port `$PORT` on `<HOSTNAME>` to port 6006 on your machine where `<PORT>` should be replaced by the output of the `echo $PORT` command above and `<HOSTNAME>` with the hostname printed by `hostname -s`.\n",
    "\n",
    "```console\n",
    "$ ssh -N -L 6006:<HOSTNAME>:<PORT> bc4-external\n",
    "```\n",
    "\n",
    "Open up http://localhost:6006. Have a look at the loss and accuracy curves plotted for both training and test.\n",
    "\n",
    "Make sure your tensorboard settings match those in the screenshot below:\n",
    "\n",
    "![Tensorboard settings](./media/tensorboard-settings.png)\n",
    "\n",
    "Your network should produce a similar result to the graphs shown below:\n",
    "\n",
    "If you see multiple curves, it's because you've trained the model multiple times already. Each run will produce a new log folder within `logs`, clear all but the most recent one and rerun tensorboard.\n",
    "\n",
    "![Expected accuracy](./media/expected-accuracy.png) ![Expected loss](./media/expected-loss.png)\n",
    "\n",
    "**Task 13:** Run `python train_cifar.py --help` and investigate the hyperparameters that you can tweak. Pick one hyperparameter to change, run the code again and look at how the loss and accuracy curves differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC4 Batch mode\n",
    "\n",
    "We provide you with the file `train_cifar.sh`, this is a file that slurm, BC4's job scheduling system, can execute to run your python script.\n",
    "The script has the following contents,\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "#SBATCH --job-name=lab2\n",
    "#SBATCH --partition teach_gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -o ./log_%j.out # STDOUT out\n",
    "#SBATCH -e ./log_%j.err # STDERR out\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --time=0:20:00\n",
    "#SBATCH --mem 4GB\n",
    "\n",
    "# get rid of any modules already loaded\n",
    "module purge\n",
    "# load in the module dependencies for this script\n",
    "module load \"languages/anaconda3/2021-3.8.8-cuda-11.1-pytorch\"\n",
    "\n",
    "python train_cifar.py\n",
    "```\n",
    "\n",
    "The lines starting with `#SBATCH` are instructions to the `sbatch` command which we use to submit the job.\n",
    "\n",
    "Exit your interactive session and submit a job using the job script from the login node:\n",
    "\n",
    "```console\n",
    "[bc4] $ sbatch train_cifar.sh  # create a new job using the job script\n",
    "[bc4] $ squeue --user $USER    # list the queued and running jobs owned by me\n",
    "JOBID         PARTITION     NAME      USER     ST       TIME  NODES NODELIST(REASON)\n",
    "2267572       gpu           train_ci  wp14832  PD       0:00      1 (Priority)\n",
    "```\n",
    "\n",
    "The last line above queries slurm asking which jobs in the queue belong to your user account. You can run the same `squeue` command to check the state of your job, it should change from `PD` to `R` when it is running, and then it'll disappear once complete.\n",
    "\n",
    "Once your job has begun to run, it will produce an output file with the name `slurm-<jobid>.out` in the same directory from which you submitted it. In my case, my job had the ID `2267572`, so my output log was called `slurm-2267572.out`. This is written to interactively, so you can watch the results of your code as it runs by running `tail -f slurm-<jobid>.out` (press `<Ctrl>-c` to quit `tail`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
